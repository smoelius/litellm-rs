//! Vertex AI Fine-tuning Module

use serde::{Deserialize, Serialize};
use serde_json::Value;
use std::collections::HashMap;

use super::error::VertexAIError;

/// Fine-tuning job
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FineTuningJob {
    pub name: String,
    pub display_name: String,
    pub base_model: String,
    pub state: FineTuningState,
    pub create_time: String,
    pub start_time: Option<String>,
    pub end_time: Option<String>,
    pub update_time: String,
    pub tuned_model: Option<String>,
    pub training_task: TrainingTask,
    pub error: Option<FineTuningError>,
    pub labels: HashMap<String, String>,
    pub experiment: Option<String>,
    pub tuned_model_display_name: Option<String>,
}

/// Fine-tuning job state
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum FineTuningState {
    #[serde(rename = "JOB_STATE_UNSPECIFIED")]
    Unspecified,
    #[serde(rename = "JOB_STATE_QUEUED")]
    Queued,
    #[serde(rename = "JOB_STATE_PENDING")]
    Pending,
    #[serde(rename = "JOB_STATE_RUNNING")]
    Running,
    #[serde(rename = "JOB_STATE_SUCCEEDED")]
    Succeeded,
    #[serde(rename = "JOB_STATE_FAILED")]
    Failed,
    #[serde(rename = "JOB_STATE_CANCELLING")]
    Cancelling,
    #[serde(rename = "JOB_STATE_CANCELLED")]
    Cancelled,
    #[serde(rename = "JOB_STATE_PAUSED")]
    Paused,
    #[serde(rename = "JOB_STATE_EXPIRED")]
    Expired,
}

/// Training task configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrainingTask {
    pub inputs: TrainingInputs,
    pub training_steps: Option<i64>,
    pub learning_rate_multiplier: Option<f32>,
    pub adapter_size: Option<AdapterSize>,
    pub tuning_data_stats: Option<TuningDataStats>,
}

/// Training inputs
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrainingInputs {
    pub supervised_tuning_spec: SupervisedTuningSpec,
}

/// Supervised tuning specification
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SupervisedTuningSpec {
    pub training_dataset_uri: String,
    pub validation_dataset_uri: Option<String>,
    pub hyper_parameters: Option<SupervisedHyperParameters>,
}

/// Supervised tuning hyperparameters
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SupervisedHyperParameters {
    pub epoch_count: Option<i64>,
    pub learning_rate_multiplier: Option<f32>,
    pub adapter_size: Option<AdapterSize>,
}

/// Adapter size for fine-tuning
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AdapterSize {
    #[serde(rename = "ADAPTER_SIZE_UNSPECIFIED")]
    Unspecified,
    #[serde(rename = "ADAPTER_SIZE_ONE")]
    One,
    #[serde(rename = "ADAPTER_SIZE_FOUR")]
    Four,
    #[serde(rename = "ADAPTER_SIZE_EIGHT")]
    Eight,
    #[serde(rename = "ADAPTER_SIZE_SIXTEEN")]
    Sixteen,
    #[serde(rename = "ADAPTER_SIZE_THIRTY_TWO")]
    ThirtyTwo,
}

/// Tuning data statistics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TuningDataStats {
    pub supervised_tuning_data_stats: Option<SupervisedTuningDataStats>,
}

/// Supervised tuning data statistics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SupervisedTuningDataStats {
    pub tuning_dataset_example_count: i64,
    pub total_tuning_character_count: i64,
    pub tuning_step_count: i64,
    pub user_input_token_distribution: Option<DatasetDistribution>,
    pub user_output_token_distribution: Option<DatasetDistribution>,
    pub user_message_per_example_distribution: Option<DatasetDistribution>,
    pub user_dataset_examples: Vec<SupervisedTuningDatasetExample>,
}

/// Dataset distribution
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DatasetDistribution {
    pub min: f32,
    pub max: f32,
    pub mean: f32,
    pub median: f32,
    pub p5: f32,
    pub p95: f32,
    pub buckets: Vec<DatasetDistributionBucket>,
}

/// Dataset distribution bucket
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DatasetDistributionBucket {
    pub count: i64,
    pub left: f32,
    pub right: f32,
}

/// Supervised tuning dataset example
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SupervisedTuningDatasetExample {
    pub user_input: String,
    pub user_output: String,
}

/// Fine-tuning error
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FineTuningError {
    pub code: i32,
    pub message: String,
    pub details: Option<Vec<Value>>,
}

/// Create fine-tuning job request
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CreateFineTuningJobRequest {
    pub display_name: String,
    pub base_model: String,
    pub training_dataset_uri: String,
    pub validation_dataset_uri: Option<String>,
    pub tuned_model_display_name: Option<String>,
    pub epoch_count: Option<i64>,
    pub learning_rate_multiplier: Option<f32>,
    pub adapter_size: Option<AdapterSize>,
}

/// Fine-tuning job list response
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ListFineTuningJobsResponse {
    pub tuning_jobs: Vec<FineTuningJob>,
    pub next_page_token: Option<String>,
}

/// Fine-tuning handler
pub struct FineTuningHandler {
    project_id: String,
    location: String,
}

impl FineTuningHandler {
    /// Create new fine-tuning handler
    pub fn new(project_id: String, location: String) -> Self {
        Self {
            project_id,
            location,
        }
    }

    /// Create a fine-tuning job
    pub async fn create_tuning_job(
        &self,
        request: CreateFineTuningJobRequest,
    ) -> Result<FineTuningJob, VertexAIError> {
        // Validate request
        self.validate_tuning_request(&request)?;

        // TODO: Implement actual fine-tuning job creation
        Ok(FineTuningJob {
            name: format!(
                "projects/{}/locations/{}/tuningJobs/{}",
                self.project_id,
                self.location,
                uuid::Uuid::new_v4()
            ),
            display_name: request.display_name,
            base_model: request.base_model,
            state: FineTuningState::Queued,
            create_time: chrono::Utc::now().to_rfc3339(),
            start_time: None,
            end_time: None,
            update_time: chrono::Utc::now().to_rfc3339(),
            tuned_model: None,
            training_task: TrainingTask {
                inputs: TrainingInputs {
                    supervised_tuning_spec: SupervisedTuningSpec {
                        training_dataset_uri: request.training_dataset_uri,
                        validation_dataset_uri: request.validation_dataset_uri,
                        hyper_parameters: Some(SupervisedHyperParameters {
                            epoch_count: request.epoch_count,
                            learning_rate_multiplier: request.learning_rate_multiplier,
                            adapter_size: request.adapter_size.clone(),
                        }),
                    },
                },
                training_steps: None,
                learning_rate_multiplier: request.learning_rate_multiplier,
                adapter_size: request.adapter_size,
                tuning_data_stats: None,
            },
            error: None,
            labels: HashMap::new(),
            experiment: None,
            tuned_model_display_name: request.tuned_model_display_name,
        })
    }

    /// Get fine-tuning job status
    pub async fn get_tuning_job(&self, _job_id: &str) -> Result<FineTuningJob, VertexAIError> {
        // TODO: Implement actual job retrieval
        Err(VertexAIError::UnsupportedFeature(
            "Fine-tuning job retrieval not yet implemented".to_string(),
        ))
    }

    /// List fine-tuning jobs
    pub async fn list_tuning_jobs(
        &self,
        _filter: Option<String>,
        _page_size: Option<i32>,
        _page_token: Option<String>,
    ) -> Result<ListFineTuningJobsResponse, VertexAIError> {
        // TODO: Implement actual job listing
        Ok(ListFineTuningJobsResponse {
            tuning_jobs: Vec::new(),
            next_page_token: None,
        })
    }

    /// Cancel a fine-tuning job
    pub async fn cancel_tuning_job(&self, _job_id: &str) -> Result<(), VertexAIError> {
        // TODO: Implement actual job cancellation
        Ok(())
    }

    /// Delete a fine-tuning job
    pub async fn delete_tuning_job(&self, _job_id: &str) -> Result<(), VertexAIError> {
        // TODO: Implement actual job deletion
        Ok(())
    }

    /// Validate fine-tuning request
    fn validate_tuning_request(
        &self,
        request: &CreateFineTuningJobRequest,
    ) -> Result<(), VertexAIError> {
        // Check if base model supports fine-tuning
        if !self.is_tunable_model(&request.base_model) {
            return Err(VertexAIError::InvalidRequest(format!(
                "Model {} does not support fine-tuning",
                request.base_model
            )));
        }

        // Validate dataset URI
        if !request.training_dataset_uri.starts_with("gs://") {
            return Err(VertexAIError::InvalidRequest(
                "Training dataset must be a GCS URI (gs://)".to_string(),
            ));
        }

        // Validate hyperparameters
        if let Some(epochs) = request.epoch_count {
            if !(1..=100).contains(&epochs) {
                return Err(VertexAIError::InvalidRequest(
                    "Epoch count must be between 1 and 100".to_string(),
                ));
            }
        }

        if let Some(lr_mult) = request.learning_rate_multiplier {
            if lr_mult <= 0.0 || lr_mult > 10.0 {
                return Err(VertexAIError::InvalidRequest(
                    "Learning rate multiplier must be between 0 and 10".to_string(),
                ));
            }
        }

        Ok(())
    }

    /// Check if model supports fine-tuning
    fn is_tunable_model(&self, model: &str) -> bool {
        // List of models that support fine-tuning
        matches!(
            model,
            "gemini-1.0-pro-002"
                | "gemini-1.5-pro-002"
                | "gemini-1.5-flash-002"
                | "text-bison@002"
                | "code-bison@002"
        )
    }

    /// Get recommended hyperparameters for a model
    pub fn get_recommended_hyperparameters(&self, model: &str) -> SupervisedHyperParameters {
        match model {
            model if model.contains("gemini") => SupervisedHyperParameters {
                epoch_count: Some(3),
                learning_rate_multiplier: Some(1.0),
                adapter_size: Some(AdapterSize::Four),
            },
            model if model.contains("bison") => SupervisedHyperParameters {
                epoch_count: Some(5),
                learning_rate_multiplier: Some(0.5),
                adapter_size: Some(AdapterSize::Eight),
            },
            _ => SupervisedHyperParameters {
                epoch_count: Some(3),
                learning_rate_multiplier: Some(1.0),
                adapter_size: Some(AdapterSize::Four),
            },
        }
    }

    /// Estimate fine-tuning cost
    pub fn estimate_tuning_cost(&self, model: &str, training_examples: usize, epochs: i64) -> f64 {
        // Rough cost estimation (in USD)
        let base_cost_per_example = match model {
            model if model.contains("gemini-1.5-pro") => 0.005,
            model if model.contains("gemini-1.5-flash") => 0.002,
            model if model.contains("gemini-1.0-pro") => 0.003,
            _ => 0.002,
        };

        let total_examples = training_examples as f64 * epochs as f64;
        total_examples * base_cost_per_example
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_is_tunable_model() {
        let handler = FineTuningHandler::new("test".to_string(), "us-central1".to_string());

        assert!(handler.is_tunable_model("gemini-1.5-pro-002"));
        assert!(handler.is_tunable_model("text-bison@002"));
        assert!(!handler.is_tunable_model("gemini-pro"));
        assert!(!handler.is_tunable_model("claude-3-sonnet"));
    }

    #[test]
    fn test_estimate_tuning_cost() {
        let handler = FineTuningHandler::new("test".to_string(), "us-central1".to_string());

        let cost = handler.estimate_tuning_cost("gemini-1.5-flash-002", 1000, 3);
        assert!(cost > 0.0);

        let pro_cost = handler.estimate_tuning_cost("gemini-1.5-pro-002", 1000, 3);
        let flash_cost = handler.estimate_tuning_cost("gemini-1.5-flash-002", 1000, 3);
        assert!(pro_cost > flash_cost);
    }

    #[test]
    fn test_validate_tuning_request() {
        let handler = FineTuningHandler::new("test".to_string(), "us-central1".to_string());

        let valid_request = CreateFineTuningJobRequest {
            display_name: "Test Job".to_string(),
            base_model: "gemini-1.5-flash-002".to_string(),
            training_dataset_uri: "gs://my-bucket/training.jsonl".to_string(),
            validation_dataset_uri: None,
            tuned_model_display_name: None,
            epoch_count: Some(3),
            learning_rate_multiplier: Some(1.0),
            adapter_size: Some(AdapterSize::Four),
        };

        assert!(handler.validate_tuning_request(&valid_request).is_ok());

        let invalid_request = CreateFineTuningJobRequest {
            base_model: "unsupported-model".to_string(),
            training_dataset_uri: "/local/file.jsonl".to_string(),
            ..valid_request
        };

        assert!(handler.validate_tuning_request(&invalid_request).is_err());
    }
}
