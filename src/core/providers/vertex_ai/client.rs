//! Vertex AI Client Implementation

use async_trait::async_trait;
use reqwest::{Client, Response};
use serde_json::Value;
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::RwLock;
use tracing::debug;

use crate::core::{
    traits::{ErrorMapper, provider::LLMProvider},
    types::{
        common::{HealthStatus, ModelInfo, ProviderCapability, RequestContext},
        requests::{ChatRequest, EmbeddingRequest, ImageGenerationRequest},
        responses::{ChatResponse, EmbeddingResponse, ImageGenerationResponse},
    },
};
use std::collections::HashMap;

use super::{
    VertexAIProviderConfig,
    auth::VertexAuth,
    error::VertexAIError,
    models::VertexAIModel,
    transformers::{GeminiTransformer, PartnerModelTransformer},
};

// Cost calculation removed - integrated in provider implementation

/// VertexAI-specific error mapper implementation
#[derive(Debug)]
pub struct VertexAIErrorMapper;

impl ErrorMapper<VertexAIError> for VertexAIErrorMapper {
    fn map_http_error(&self, status_code: u16, response_body: &str) -> VertexAIError {
        match status_code {
            400 => VertexAIError::ResponseParsing(format!("Bad request: {}", response_body)),
            401 => VertexAIError::Authentication("Invalid credentials or API key".to_string()),
            403 => VertexAIError::Configuration(
                "Access forbidden: insufficient permissions".to_string(),
            ),
            404 => VertexAIError::UnsupportedModel("Model not found".to_string()),
            429 => VertexAIError::Network("Rate limit exceeded".to_string()),
            500 => VertexAIError::Network("Internal server error".to_string()),
            502 => VertexAIError::Network("Bad gateway".to_string()),
            503 => VertexAIError::Network("Service unavailable".to_string()),
            _ => VertexAIError::Network(format!("HTTP error {}: {}", status_code, response_body)),
        }
    }

    fn map_json_error(&self, error_response: &Value) -> VertexAIError {
        if let Some(error) = error_response.get("error") {
            let error_code = error.get("code").and_then(|c| c.as_u64()).unwrap_or(0);
            let error_message = error
                .get("message")
                .and_then(|m| m.as_str())
                .unwrap_or("Unknown error");
            let status = error
                .get("status")
                .and_then(|s| s.as_str())
                .unwrap_or("UNKNOWN");

            match status {
                "INVALID_ARGUMENT" => VertexAIError::ResponseParsing(error_message.to_string()),
                "UNAUTHENTICATED" => {
                    VertexAIError::Authentication("Authentication failed".to_string())
                }
                "PERMISSION_DENIED" => {
                    VertexAIError::Configuration("Permission denied".to_string())
                }
                "NOT_FOUND" => VertexAIError::UnsupportedModel(error_message.to_string()),
                "RESOURCE_EXHAUSTED" => {
                    VertexAIError::Network("Rate limit or quota exceeded".to_string())
                }
                "INTERNAL" | "UNAVAILABLE" => VertexAIError::Network(error_message.to_string()),
                _ => {
                    VertexAIError::Network(format!("API Error ({}): {}", error_code, error_message))
                }
            }
        } else {
            VertexAIError::ResponseParsing("Unknown error response format".to_string())
        }
    }

    fn map_network_error(&self, error: &dyn std::error::Error) -> VertexAIError {
        VertexAIError::Network(format!("Network error: {}", error))
    }
}

/// Vertex AI Provider implementation
#[derive(Debug)]
pub struct VertexAIProvider {
    config: VertexAIProviderConfig,
    auth: Arc<VertexAuth>,
    http_client: Client,
    // Cost calculation integrated internally
    health_status: Arc<RwLock<HealthStatus>>,
    gemini_transformer: GeminiTransformer,
    partner_transformer: PartnerModelTransformer,
}

impl VertexAIProvider {
    /// Create a new Vertex AI provider
    pub async fn new(config: VertexAIProviderConfig) -> Result<Self, VertexAIError> {
        let auth = Arc::new(VertexAuth::new(config.credentials.clone()));

        let http_client = Client::builder()
            .timeout(Duration::from_secs(config.timeout_seconds))
            .build()
            .map_err(|e| VertexAIError::Configuration(e.to_string()))?;

        // Cost calculation integrated in provider implementation
        let health_status = Arc::new(RwLock::new(HealthStatus::Healthy));

        Ok(Self {
            config,
            auth,
            http_client,
            health_status,
            gemini_transformer: GeminiTransformer::new(),
            partner_transformer: PartnerModelTransformer::new(),
        })
    }

    /// Build the API URL for a given model and endpoint
    fn build_url(&self, model: &VertexAIModel, endpoint: &str, stream: bool) -> String {
        let model_id = model.model_id();
        let location = &self.config.location;
        let project_id = &self.config.project_id;
        let api_version = &self.config.api_version;

        // Handle custom API base
        if let Some(ref api_base) = self.config.api_base {
            return format!("{}/{}:{}", api_base, model_id, endpoint);
        }

        // Special handling for global models
        let use_global = location == "global" || model_id.contains("imagen");

        let base_url = if use_global {
            format!(
                "https://aiplatform.googleapis.com/{}/projects/{}/locations/global",
                api_version, project_id
            )
        } else {
            format!(
                "https://{}-aiplatform.googleapis.com/{}/projects/{}/locations/{}",
                location, api_version, project_id, location
            )
        };

        // Build full URL based on model type
        let url = if model.is_gemini() {
            format!(
                "{}/publishers/google/models/{}:{}",
                base_url, model_id, endpoint
            )
        } else if model.is_partner_model() {
            // Partner models have different URL structure
            let publisher = self.get_publisher_for_model(&model_id);
            format!(
                "{}/publishers/{}/models/{}:{}",
                base_url, publisher, model_id, endpoint
            )
        } else {
            // Custom models
            format!("{}/endpoints/{}:{}", base_url, model_id, endpoint)
        };

        // Add streaming parameter if needed
        if stream {
            format!("{}?alt=sse", url)
        } else {
            url
        }
    }

    /// Get publisher for partner models
    fn get_publisher_for_model(&self, model_id: &str) -> &str {
        if model_id.contains("claude") {
            "anthropic"
        } else if model_id.contains("llama") {
            "meta"
        } else if model_id.contains("jamba") {
            "ai21"
        } else {
            "google"
        }
    }

    /// Make an authenticated request
    async fn make_request(&self, url: &str, body: Value) -> Result<Response, VertexAIError> {
        let token = self
            .auth
            .get_access_token()
            .await
            .map_err(|e| VertexAIError::Authentication(e.to_string()))?;

        debug!("Making request to Vertex AI: {}", url);

        let response = self
            .http_client
            .post(url)
            .header("Authorization", format!("Bearer {}", token))
            .header("Content-Type", "application/json")
            .json(&body)
            .send()
            .await
            .map_err(|e| VertexAIError::Network(e.to_string()))?;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();

            return Err(VertexAIError::ApiError {
                status_code: status.as_u16(),
                message: error_text,
            });
        }

        Ok(response)
    }

    /// Execute chat completion
    pub async fn chat_completion_internal(
        &self,
        request: ChatRequest,
        _context: RequestContext,
    ) -> Result<ChatResponse, VertexAIError> {
        let model = super::parse_vertex_model(&request.model);

        // Transform request based on model type
        let (endpoint, body) = if model.is_gemini() {
            let endpoint = if request.stream {
                "streamGenerateContent"
            } else {
                "generateContent"
            };

            let body = self
                .gemini_transformer
                .transform_chat_request(&request, &model)?;
            (endpoint, body)
        } else if model.is_partner_model() {
            // Partner models use different endpoints
            let endpoint = "predict";
            let body = self
                .partner_transformer
                .transform_chat_request(&request, &model)?;
            (endpoint, body)
        } else {
            return Err(VertexAIError::UnsupportedModel(request.model.clone()));
        };

        let url = self.build_url(&model, endpoint, request.stream);
        let response = self.make_request(&url, body).await?;

        // Parse response
        let response_body: Value = response
            .json()
            .await
            .map_err(|e| VertexAIError::ResponseParsing(e.to_string()))?;

        // Transform response back to standard format
        if model.is_gemini() {
            self.gemini_transformer
                .transform_chat_response(response_body, &model)
        } else {
            self.partner_transformer
                .transform_chat_response(response_body, &model)
        }
    }

    /// Execute embedding request
    pub async fn embedding_internal(
        &self,
        request: EmbeddingRequest,
        _context: RequestContext,
    ) -> Result<EmbeddingResponse, VertexAIError> {
        // Vertex AI uses specific embedding models
        let model_name = if request.model.contains("embedding") {
            request.model.clone()
        } else {
            "text-embedding-004".to_string() // Default embedding model
        };

        let endpoint = "predict";
        let url = format!(
            "https://{}-aiplatform.googleapis.com/v1/projects/{}/locations/{}/publishers/google/models/{}:{}",
            self.config.location,
            self.config.project_id,
            self.config.location,
            model_name,
            endpoint
        );

        // Build request body
        let instances: Vec<Value> = request
            .input
            .iter()
            .map(|text| {
                serde_json::json!({
                    "content": text,
                    "task_type": "RETRIEVAL_DOCUMENT"
                })
            })
            .collect();

        let body = serde_json::json!({
            "instances": instances
        });

        let response = self.make_request(&url, body).await?;
        let response_body: Value = response
            .json()
            .await
            .map_err(|e| VertexAIError::ResponseParsing(e.to_string()))?;

        // Parse embeddings from response
        let predictions = response_body["predictions"]
            .as_array()
            .ok_or_else(|| VertexAIError::ResponseParsing("Missing predictions".to_string()))?;

        let embeddings = predictions
            .iter()
            .enumerate()
            .map(|(index, pred)| {
                let values = pred["embeddings"]["values"]
                    .as_array()
                    .ok_or_else(|| {
                        VertexAIError::ResponseParsing("Missing embedding values".to_string())
                    })?
                    .iter()
                    .map(|v| v.as_f64().unwrap_or(0.0) as f32)
                    .collect();

                Ok(crate::core::types::responses::EmbeddingData {
                    object: "embedding".to_string(),
                    index: index as u32,
                    embedding: values,
                })
            })
            .collect::<Result<Vec<crate::core::types::responses::EmbeddingData>, VertexAIError>>(
            )?;

        Ok(EmbeddingResponse {
            object: "list".to_string(),
            data: embeddings.clone(),
            model: model_name,
            usage: None, // Vertex AI doesn't return token usage for embeddings
            embeddings: Some(embeddings), // Backward compatibility field
        })
    }

    /// Count tokens for a request
    pub async fn count_tokens(
        &self,
        model: &str,
        messages: &[Value],
    ) -> Result<usize, VertexAIError> {
        let model_obj = super::parse_vertex_model(model);
        let endpoint = "countTokens";
        let url = self.build_url(&model_obj, endpoint, false);

        let body = serde_json::json!({
            "contents": messages
        });

        let response = self.make_request(&url, body).await?;
        let response_body: Value = response
            .json()
            .await
            .map_err(|e| VertexAIError::ResponseParsing(e.to_string()))?;

        response_body["totalTokens"]
            .as_u64()
            .map(|v| v as usize)
            .ok_or_else(|| VertexAIError::ResponseParsing("Missing token count".to_string()))
    }
}

#[async_trait]
impl LLMProvider for VertexAIProvider {
    type Config = VertexAIProviderConfig;
    type Error = VertexAIError;
    type ErrorMapper = VertexAIErrorMapper;

    fn name(&self) -> &'static str {
        "vertex_ai"
    }

    fn capabilities(&self) -> &'static [crate::core::types::common::ProviderCapability] {
        use crate::core::types::common::ProviderCapability;
        &[
            ProviderCapability::ChatCompletion,
            ProviderCapability::ChatCompletionStream,
            ProviderCapability::Embeddings,
            ProviderCapability::ImageGeneration,
            ProviderCapability::ToolCalling,
        ]
    }

    fn models(&self) -> &[ModelInfo] {
        use once_cell::sync::Lazy;
        static MODELS: Lazy<Vec<ModelInfo>> = Lazy::new(|| {
            vec![
                ModelInfo {
                    id: "gemini-1.5-pro".to_string(),
                    name: "Gemini 1.5 Pro".to_string(),
                    provider: "vertex_ai".to_string(),
                    max_context_length: 2_097_152,
                    max_output_length: Some(8192),
                    supports_streaming: true,
                    supports_tools: true,
                    supports_multimodal: true,
                    input_cost_per_1k_tokens: Some(1.25),
                    output_cost_per_1k_tokens: Some(3.75),
                    currency: "USD".to_string(),
                    capabilities: vec![
                        ProviderCapability::ChatCompletion,
                        ProviderCapability::ChatCompletionStream,
                        ProviderCapability::FunctionCalling,
                        ProviderCapability::ToolCalling,
                    ],
                    created_at: None,
                    updated_at: None,
                    metadata: std::collections::HashMap::new(),
                },
                ModelInfo {
                    id: "gemini-1.5-flash".to_string(),
                    name: "Gemini 1.5 Flash".to_string(),
                    provider: "vertex_ai".to_string(),
                    max_context_length: 1_048_576,
                    max_output_length: Some(8192),
                    supports_streaming: true,
                    supports_tools: true,
                    supports_multimodal: true,
                    input_cost_per_1k_tokens: Some(0.0625),
                    output_cost_per_1k_tokens: Some(0.25),
                    currency: "USD".to_string(),
                    capabilities: vec![
                        ProviderCapability::ChatCompletion,
                        ProviderCapability::ChatCompletionStream,
                        ProviderCapability::FunctionCalling,
                        ProviderCapability::ToolCalling,
                    ],
                    created_at: None,
                    updated_at: None,
                    metadata: std::collections::HashMap::new(),
                },
            ]
        });
        &MODELS
    }

    async fn chat_completion(
        &self,
        request: ChatRequest,
        context: RequestContext,
    ) -> Result<ChatResponse, Self::Error> {
        self.chat_completion_internal(request, context).await
    }

    async fn embeddings(
        &self,
        request: EmbeddingRequest,
        context: RequestContext,
    ) -> Result<EmbeddingResponse, Self::Error> {
        self.embedding_internal(request, context).await
    }

    async fn image_generation(
        &self,
        request: ImageGenerationRequest,
        _context: RequestContext,
    ) -> Result<ImageGenerationResponse, Self::Error> {
        // Use Imagen model for image generation
        let endpoint = "predict";
        let model = "imagegeneration@006";

        let url = format!(
            "https://{}-aiplatform.googleapis.com/v1/projects/{}/locations/{}/publishers/google/models/{}:{}",
            self.config.location, self.config.project_id, self.config.location, model, endpoint
        );

        let body = serde_json::json!({
            "instances": [{
                "prompt": request.prompt
            }],
            "parameters": {
                "sampleCount": request.n.unwrap_or(1),
                "aspectRatio": request.size.as_deref().unwrap_or("1:1"),
            }
        });

        let response = self.make_request(&url, body).await?;
        let response_body: Value = response
            .json()
            .await
            .map_err(|e| VertexAIError::ResponseParsing(e.to_string()))?;

        let predictions = response_body["predictions"]
            .as_array()
            .ok_or_else(|| VertexAIError::ResponseParsing("Missing predictions".to_string()))?;

        let image_data = predictions
            .iter()
            .filter_map(|pred| pred["bytesBase64Encoded"].as_str())
            .map(|s| crate::core::types::responses::ImageData {
                url: None,
                b64_json: Some(s.to_string()),
                revised_prompt: None,
            })
            .collect();

        Ok(ImageGenerationResponse {
            created: chrono::Utc::now().timestamp() as u64,
            data: image_data,
        })
    }

    async fn health_check(&self) -> HealthStatus {
        match self.check_health().await {
            Ok(()) => HealthStatus::Healthy,
            Err(_) => HealthStatus::Unhealthy,
        }
    }

    async fn calculate_cost(
        &self,
        model: &str,
        input_tokens: u32,
        output_tokens: u32,
    ) -> Result<f64, Self::Error> {
        // Basic cost calculation for Vertex AI models (per 1M tokens)
        let cost = match model {
            m if m.contains("gemini-pro") => {
                (input_tokens as f64 * 0.0005 + output_tokens as f64 * 0.0015) / 1000.0
            }
            m if m.contains("gemini-1.5-pro") => {
                (input_tokens as f64 * 0.00125 + output_tokens as f64 * 0.00375) / 1000.0
            }
            m if m.contains("gemini-1.5-flash") => {
                (input_tokens as f64 * 0.000075 + output_tokens as f64 * 0.0003) / 1000.0
            }
            _ => 0.0, // Default cost for unknown models
        };
        Ok(cost)
    }

    /// Model
    fn get_supported_openai_params(&self, model: &str) -> &'static [&'static str] {
        // VertexAI supports OpenAI-compatible parameters for Gemini models
        if model.contains("gemini") {
            &[
                "messages",
                "model",
                "max_tokens",
                "temperature",
                "top_p",
                "stop",
                "stream",
                "tools",
                "tool_choice",
                "response_format",
                "user",
                "top_k",
            ]
        } else {
            // Partner models have limited OpenAI compatibility
            &[
                "messages",
                "model",
                "max_tokens",
                "temperature",
                "top_p",
                "stream",
            ]
        }
    }

    /// Map OpenAI format parameters to VertexAI API parameter format
    async fn map_openai_params(
        &self,
        params: HashMap<String, Value>,
        model: &str,
    ) -> std::result::Result<HashMap<String, Value>, Self::Error> {
        let mut vertex_params = HashMap::new();
        let vertex_model = super::parse_vertex_model(model);

        // Basic parameter mapping
        if let Some(messages) = params.get("messages") {
            vertex_params.insert("contents".to_string(), messages.clone());
        }

        vertex_params.insert("model".to_string(), Value::String(vertex_model.model_id()));

        // Generation parameter mapping
        let mut generation_config = serde_json::Map::new();

        if let Some(max_tokens) = params.get("max_tokens") {
            generation_config.insert("maxOutputTokens".to_string(), max_tokens.clone());
        }

        if let Some(temperature) = params.get("temperature") {
            generation_config.insert("temperature".to_string(), temperature.clone());
        }

        if let Some(top_p) = params.get("top_p") {
            generation_config.insert("topP".to_string(), top_p.clone());
        }

        if let Some(top_k) = params.get("top_k") {
            generation_config.insert("topK".to_string(), top_k.clone());
        }

        if let Some(stop) = params.get("stop") {
            match stop {
                Value::String(s) => {
                    generation_config.insert(
                        "stopSequences".to_string(),
                        Value::Array(vec![Value::String(s.clone())]),
                    );
                }
                Value::Array(_arr) => {
                    generation_config.insert("stopSequences".to_string(), stop.clone());
                }
                _ => {
                    return Err(VertexAIError::InvalidRequest(
                        "stop must be string or array".to_string(),
                    ));
                }
            }
        }

        if !generation_config.is_empty() {
            vertex_params.insert(
                "generationConfig".to_string(),
                Value::Object(generation_config),
            );
        }

        // tool_callparameter
        if let Some(tools) = params.get("tools") {
            vertex_params.insert("tools".to_string(), tools.clone());
        }

        if let Some(tool_choice) = params.get("tool_choice") {
            vertex_params.insert(
                "toolConfig".to_string(),
                serde_json::json!({
                    "functionCallingConfig": {
                        "mode": match tool_choice.as_str() {
                            Some("auto") => "AUTO",
                            Some("none") => "NONE",
                            _ => "AUTO"
                        }
                    }
                }),
            );
        }

        Ok(vertex_params)
    }

    /// Request
    async fn transform_request(
        &self,
        request: ChatRequest,
        _context: RequestContext,
    ) -> std::result::Result<Value, Self::Error> {
        let mut params = HashMap::new();

        params.insert(
            "messages".to_string(),
            serde_json::to_value(request.messages).unwrap(),
        );
        params.insert("model".to_string(), Value::String(request.model.clone()));

        if let Some(max_tokens) = request.max_tokens {
            params.insert(
                "max_tokens".to_string(),
                Value::Number(serde_json::Number::from(max_tokens)),
            );
        }

        if let Some(temperature) = request.temperature {
            params.insert(
                "temperature".to_string(),
                serde_json::Number::from_f64(temperature as f64)
                    .map(Value::Number)
                    .unwrap_or(Value::Null),
            );
        }

        if let Some(top_p) = request.top_p {
            params.insert(
                "top_p".to_string(),
                serde_json::Number::from_f64(top_p as f64)
                    .map(Value::Number)
                    .unwrap_or(Value::Null),
            );
        }

        if let Some(stop) = request.stop {
            params.insert("stop".to_string(), serde_json::to_value(stop).unwrap());
        }

        if request.stream {
            params.insert("stream".to_string(), Value::Bool(true));
        }

        if let Some(tools) = request.tools {
            params.insert("tools".to_string(), serde_json::to_value(tools).unwrap());
        }

        if let Some(tool_choice) = request.tool_choice {
            params.insert(
                "tool_choice".to_string(),
                serde_json::to_value(tool_choice).unwrap(),
            );
        }

        let vertex_params = self.map_openai_params(params, &request.model).await?;
        Ok(serde_json::to_value(vertex_params).unwrap())
    }

    /// Response
    async fn transform_response(
        &self,
        raw_response: &[u8],
        model: &str,
        _request_id: &str,
    ) -> std::result::Result<ChatResponse, Self::Error> {
        let response_str = std::str::from_utf8(raw_response)
            .map_err(|e| VertexAIError::ResponseParsing(format!("Invalid UTF-8: {}", e)))?;

        let response_json: Value = serde_json::from_str(response_str)
            .map_err(|e| VertexAIError::ResponseParsing(format!("JSON parsing error: {}", e)))?;

        // Error
        if let Some(_error) = response_json.get("error") {
            let error_mapper = self.get_error_mapper();
            return Err(error_mapper.map_json_error(&response_json));
        }

        // Response
        let candidates = response_json
            .get("candidates")
            .and_then(|c| c.as_array())
            .ok_or_else(|| {
                VertexAIError::ResponseParsing("Missing candidates in response".to_string())
            })?;

        if candidates.is_empty() {
            return Err(VertexAIError::ResponseParsing(
                "No candidates in response".to_string(),
            ));
        }

        let candidate = &candidates[0];
        let content = candidate
            .get("content")
            .and_then(|c| c.get("parts"))
            .and_then(|p| p.as_array())
            .and_then(|parts| parts.first())
            .and_then(|part| part.get("text"))
            .and_then(|t| t.as_str())
            .unwrap_or_default()
            .to_string();

        // Usage statistics information
        let usage = response_json.get("usageMetadata").map(|usage_json| {
            let input_tokens = usage_json
                .get("promptTokenCount")
                .and_then(|v| v.as_u64())
                .unwrap_or(0) as u32;
            let output_tokens = usage_json
                .get("candidatesTokenCount")
                .and_then(|v| v.as_u64())
                .unwrap_or(0) as u32;
            crate::core::types::responses::Usage {
                prompt_tokens: input_tokens,
                completion_tokens: output_tokens,
                total_tokens: input_tokens + output_tokens,
                prompt_tokens_details: None,
                completion_tokens_details: None,
            }
        });

        Ok(ChatResponse {
            id: format!("vertex-ai-{}", uuid::Uuid::new_v4()),
            object: "chat.completion".to_string(),
            created: chrono::Utc::now().timestamp(),
            model: model.to_string(),
            choices: vec![crate::core::types::responses::ChatChoice {
                index: 0,
                message: crate::core::types::requests::ChatMessage {
                    role: crate::core::types::requests::MessageRole::Assistant,
                    content: Some(crate::core::types::requests::MessageContent::Text(content)),
                    name: None,
                    tool_calls: None, // Handle
                    tool_call_id: None,
                    function_call: None,
                },
                finish_reason: candidate
                    .get("finishReason")
                    .and_then(|r| r.as_str())
                    .map(|reason| match reason {
                        "STOP" => crate::core::types::responses::FinishReason::Stop,
                        "MAX_TOKENS" => crate::core::types::responses::FinishReason::Length,
                        "SAFETY" => crate::core::types::responses::FinishReason::ContentFilter,
                        "RECITATION" => crate::core::types::responses::FinishReason::ContentFilter,
                        _ => crate::core::types::responses::FinishReason::Stop,
                    })
                    .or(Some(crate::core::types::responses::FinishReason::Stop)),
                logprobs: None,
            }],
            usage,
            system_fingerprint: None,
        })
    }

    /// Error
    fn get_error_mapper(&self) -> Self::ErrorMapper {
        VertexAIErrorMapper
    }
}

impl VertexAIProvider {
    /// Internal health check
    async fn check_health(&self) -> Result<(), VertexAIError> {
        // Simple health check by calling countTokens
        let url = format!(
            "https://{}-aiplatform.googleapis.com/v1/projects/{}/locations/{}/publishers/google/models/gemini-1.5-flash:countTokens",
            self.config.location, self.config.project_id, self.config.location
        );

        let body = serde_json::json!({
            "contents": [{
                "parts": [{"text": "test"}]
            }]
        });

        self.make_request(&url, body).await?;
        Ok(())
    }
}
