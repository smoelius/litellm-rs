//! Azure AI Provider (Foundry)
//!
//! Complete Azure AI Foundry integration following unified provider architecture

// Complete Azure AI modules
pub mod chat;
pub mod config;
pub mod embed;
pub mod error;
pub mod image_generation;
pub mod models;
pub mod rerank;

// Keep simplified chat for backward compatibility during transition
mod chat_simple;

// Re-export main components
pub use chat::{AzureAIChatHandler, AzureAIChatUtils};
pub use config::{AzureAIConfig, AzureAIEndpointType};
pub use embed::{AzureAIEmbeddingHandler, AzureAIEmbeddingUtils};
pub use error::AzureAIErrorMapper;
pub use image_generation::AzureAIImageHandler;
pub use models::{AzureAIModelRegistry, AzureAIModelSpec, AzureAIModelType, get_azure_ai_registry};
pub use rerank::{AzureAIRerankHandler, AzureAIRerankUtils, RerankRequest, RerankResponse};

use async_trait::async_trait;
use futures::Stream;
use serde_json::Value;
use std::collections::HashMap;
use std::pin::Pin;

use crate::core::providers::unified_provider::ProviderError;
use crate::core::traits::provider::llm_provider::trait_definition::LLMProvider;
use crate::core::types::{
    common::{HealthStatus, ModelInfo, ProviderCapability, RequestContext},
    requests::{ChatRequest, EmbeddingRequest, ImageGenerationRequest},
    responses::{ChatChunk, ChatResponse, EmbeddingResponse, ImageGenerationResponse},
};

/// Main Azure AI provider following unified architecture
#[derive(Debug, Clone)]
pub struct AzureAIProvider {
    config: AzureAIConfig,
    chat_handler: AzureAIChatHandler,
    embedding_handler: AzureAIEmbeddingHandler,
    image_handler: AzureAIImageHandler,
    rerank_handler: AzureAIRerankHandler,
    model_registry: &'static AzureAIModelRegistry,
}

impl AzureAIProvider {
    /// Create new Azure AI provider
    pub fn new(config: AzureAIConfig) -> Result<Self, ProviderError> {
        // Configuration
        config
            .validate()
            .map_err(|e| ProviderError::configuration("azure_ai", &e))?;

        let chat_handler = AzureAIChatHandler::new(config.clone())
            .map_err(|e| ProviderError::configuration("azure_ai", e.to_string()))?;
        let embedding_handler = AzureAIEmbeddingHandler::new(config.clone())
            .map_err(|e| ProviderError::configuration("azure_ai", e.to_string()))?;
        let image_handler = AzureAIImageHandler::new(config.clone())
            .map_err(|e| ProviderError::configuration("azure_ai", e.to_string()))?;
        let rerank_handler = AzureAIRerankHandler::new(config.clone())
            .map_err(|e| ProviderError::configuration("azure_ai", e.to_string()))?;
        let model_registry = get_azure_ai_registry();

        Ok(Self {
            config,
            chat_handler,
            embedding_handler,
            image_handler,
            rerank_handler,
            model_registry,
        })
    }

    /// Get Azure AI configuration
    pub fn get_config(&self) -> &AzureAIConfig {
        &self.config
    }

    /// Create from environment variables
    pub fn from_env() -> Result<Self, ProviderError> {
        let config = AzureAIConfig::from_env();
        Self::new(config)
    }

    /// Create with API key
    pub fn with_api_key(
        api_key: impl Into<String>,
        api_base: impl Into<String>,
    ) -> Result<Self, ProviderError> {
        let mut config = AzureAIConfig::new("azure_ai");
        config.base.api_key = Some(api_key.into());
        config.base.api_base = Some(api_base.into());
        Self::new(config)
    }

    /// Get chat handler
    pub fn get_chat_handler(&self) -> &AzureAIChatHandler {
        &self.chat_handler
    }

    /// Get embedding handler
    pub fn get_embedding_handler(&self) -> &AzureAIEmbeddingHandler {
        &self.embedding_handler
    }

    /// Get image generation handler
    pub fn get_image_handler(&self) -> &AzureAIImageHandler {
        &self.image_handler
    }

    /// Get reranking handler
    pub fn get_rerank_handler(&self) -> &AzureAIRerankHandler {
        &self.rerank_handler
    }

    /// Get model registry
    pub fn get_model_registry(&self) -> &AzureAIModelRegistry {
        self.model_registry
    }
}

#[async_trait]
impl LLMProvider for AzureAIProvider {
    type Config = AzureAIConfig;
    type Error = ProviderError;
    type ErrorMapper = AzureAIErrorMapper;

    fn name(&self) -> &'static str {
        "azure_ai"
    }

    fn capabilities(&self) -> &'static [ProviderCapability] {
        &[
            ProviderCapability::ChatCompletion,
            ProviderCapability::ChatCompletionStream,
            ProviderCapability::Embeddings,
            ProviderCapability::ImageGeneration,
        ]
    }

    fn models(&self) -> &[ModelInfo] {
        // Get
        static MODELS: std::sync::OnceLock<Vec<ModelInfo>> = std::sync::OnceLock::new();
        MODELS.get_or_init(|| self.model_registry.to_model_infos())
    }

    fn get_supported_openai_params(&self, _model: &str) -> &'static [&'static str] {
        &[
            "temperature",
            "max_tokens",
            "max_completion_tokens",
            "top_p",
            "frequency_penalty",
            "presence_penalty",
            "tools",
            "tool_choice",
            "stream",
        ]
    }

    async fn map_openai_params(
        &self,
        params: HashMap<String, Value>,
        _model: &str,
    ) -> Result<HashMap<String, Value>, Self::Error> {
        // Azure AI generally uses the same parameters as OpenAI
        Ok(params)
    }

    async fn transform_request(
        &self,
        request: ChatRequest,
        _context: RequestContext,
    ) -> Result<Value, Self::Error> {
        // Transform ChatRequest to Azure AI API format
        AzureAIChatUtils::transform_request(&request)
    }

    async fn transform_response(
        &self,
        raw_response: &[u8],
        model: &str,
        _request_id: &str,
    ) -> Result<ChatResponse, Self::Error> {
        // Parse Azure AI response to ChatResponse
        let response_json: Value = serde_json::from_slice(raw_response)
            .map_err(|e| ProviderError::response_parsing("azure_ai", e.to_string()))?;

        AzureAIChatUtils::transform_response(response_json, model)
    }

    fn get_error_mapper(&self) -> Self::ErrorMapper {
        AzureAIErrorMapper
    }

    async fn chat_completion(
        &self,
        request: ChatRequest,
        context: RequestContext,
    ) -> Result<ChatResponse, Self::Error> {
        self.chat_handler
            .create_chat_completion(request, context)
            .await
    }

    async fn chat_completion_stream(
        &self,
        request: ChatRequest,
        context: RequestContext,
    ) -> Result<Pin<Box<dyn Stream<Item = Result<ChatChunk, Self::Error>> + Send>>, Self::Error>
    {
        let stream = self
            .chat_handler
            .create_chat_completion_stream(request, context)
            .await?;
        Ok(Box::pin(stream))
    }

    async fn embeddings(
        &self,
        request: EmbeddingRequest,
        context: RequestContext,
    ) -> Result<EmbeddingResponse, Self::Error> {
        self.embedding_handler.embedding(request, context).await
    }

    async fn image_generation(
        &self,
        request: ImageGenerationRequest,
        context: RequestContext,
    ) -> Result<ImageGenerationResponse, Self::Error> {
        self.image_handler.generate_image(request, context).await
    }

    async fn health_check(&self) -> HealthStatus {
        // Validate configuration first
        if self.config.validate().is_err() {
            return HealthStatus::Unhealthy;
        }

        // Try a simple ping to the API
        // For now just return healthy if config is valid
        HealthStatus::Healthy
    }

    async fn calculate_cost(
        &self,
        model: &str,
        input_tokens: u32,
        output_tokens: u32,
    ) -> Result<f64, Self::Error> {
        if let Some(model_spec) = self.model_registry.get_model(model) {
            let input_cost =
                model_spec.input_price_per_1k.unwrap_or(0.0) * (input_tokens as f64 / 1000.0);
            let output_cost =
                model_spec.output_price_per_1k.unwrap_or(0.0) * (output_tokens as f64 / 1000.0);
            Ok(input_cost + output_cost)
        } else {
            Err(ProviderError::model_not_found(
                "azure_ai",
                "Model not found for cost calculation",
            ))
        }
    }
}

/// Azure AI provider factory
pub struct AzureAIProviderFactory;

impl AzureAIProviderFactory {
    /// Create provider with default configuration
    pub fn create_default() -> Result<AzureAIProvider, ProviderError> {
        let config = AzureAIConfig::new("azure_ai");
        AzureAIProvider::new(config)
    }

    /// Create provider with custom configuration
    pub fn create_with_config(config: AzureAIConfig) -> Result<AzureAIProvider, ProviderError> {
        AzureAIProvider::new(config)
    }

    /// Create provider from environment variables
    pub fn create_from_env() -> Result<AzureAIProvider, ProviderError> {
        let config = AzureAIConfig::from_env();
        AzureAIProvider::new(config)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_provider_factory() {
        // This will fail without proper env vars, but tests the structure
        let _result = AzureAIProviderFactory::create_default();
    }

    #[tokio::test]
    async fn test_provider_creation() {
        let config = AzureAIConfig::new("azure_ai");
        // This will fail without proper env vars, but tests the structure
        let _result = AzureAIProvider::new(config);
    }

    #[test]
    fn test_model_capabilities() {
        let registry = get_azure_ai_registry();

        assert!(registry.supports_capability("gpt-4o", &ProviderCapability::ChatCompletion));
        assert!(
            registry.supports_capability("text-embedding-3-large", &ProviderCapability::Embeddings)
        );
        assert!(!registry.supports_capability("dall-e-3", &ProviderCapability::ChatCompletion));
    }
}
